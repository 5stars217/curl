{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification with Encrypted Neural Networks\n",
    "\n",
    "In this tutorial, we'll look at how we can achieve the <i>Model Hiding</i> application we discussed in the Introduction. That is, suppose say Alice has a trained model she wishes to keep private, and Bob has some data he wishes to classify while keeping it private. We will see how CrypTen allows Alice and Bob to coordinate and classify the data, while achieving their privacy requirements.\n",
    "\n",
    "To simulate this scenario, we will begin with Alice training a simple neural network on MNIST data. Then we'll see how Alice and Bob encrypt their network and data respectively, classify the encrypted data and finally decrypt the labels.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We first import the `torch` and `crypten` libraries, and initialize `crypten`. We will use a helper script `mnist_utils.py` to split the public MNIST data into Alice's portion and Bob's portion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import crypten.nn as nn\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "crypten.init()\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.3749, -1.7004, -0.7123, -0.6363, -0.0685, -0.0942, -0.5310,  3.0206,\n",
      "         -0.3940, -1.6077],\n",
      "        [ 0.8402, -1.5226, -0.7774, -0.9728,  0.2623,  1.9485, -1.1585,  1.8922,\n",
      "          0.7155,  0.1293],\n",
      "        [ 0.7431,  1.7675,  0.0258,  0.6285,  1.3089,  0.3253, -2.4253,  0.3428,\n",
      "         -1.0613,  1.7637],\n",
      "        [-0.1879, -0.9199,  1.0139,  2.8269,  1.6581, -1.7827,  0.8733,  1.2514,\n",
      "         -1.3100, -2.0500],\n",
      "        [-0.3762, -1.0989,  1.9507, -0.7418,  0.3374, -0.7515,  0.3272, -0.1313,\n",
      "         -0.8547,  1.1360]], requires_grad=True)\n",
      "tensor([[ 0.8402, -1.5226, -0.7774, -0.9728,  0.2623,  1.9484, -1.1585,  1.8922,\n",
      "          0.7155,  0.1293],\n",
      "        [ 0.7430,  1.7675,  0.0258,  0.6285,  1.3089,  0.3253, -2.4253,  0.3428,\n",
      "         -1.0613,  1.7637],\n",
      "        [-1.3748, -1.7004, -0.7123, -0.6362, -0.0685, -0.0942, -0.5310,  3.0206,\n",
      "         -0.3940, -1.6077],\n",
      "        [ 0.8402, -1.5226, -0.7774, -0.9728,  0.2623,  1.9484, -1.1585,  1.8922,\n",
      "          0.7155,  0.1293],\n",
      "        [ 0.7430,  1.7675,  0.0258,  0.6285,  1.3089,  0.3253, -2.4253,  0.3428,\n",
      "         -1.0613,  1.7637],\n",
      "        [-0.1879, -0.9199,  1.0139,  2.8269,  1.6581, -1.7827,  0.8733,  1.2514,\n",
      "         -1.3100, -2.0500],\n",
      "        [-0.3762, -1.0989,  1.9507, -0.7418,  0.3374, -0.7515,  0.3272, -0.1313,\n",
      "         -0.8547,  1.1360]])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Embedding(5, 10)\n",
    "print(layer.weight)\n",
    "layer.encrypt(src=0)\n",
    "data = torch.tensor([1, 2, 0, 1, 2, 3, 4])\n",
    "data_enc = crypten.cryptensor(data)\n",
    "output = layer.forward(data_enc)\n",
    "print(output.get_plain_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8402, -1.5226, -0.7774, -0.9728,  0.2623,  1.9484, -1.1585,  1.8922,\n",
       "          0.7155,  0.1293],\n",
       "        [ 0.7430,  1.7675,  0.0258,  0.6285,  1.3089,  0.3253, -2.4253,  0.3428,\n",
       "         -1.0613,  1.7637],\n",
       "        [-1.3748, -1.7004, -0.7123, -0.6362, -0.0685, -0.0942, -0.5310,  3.0206,\n",
       "         -0.3940, -1.6077],\n",
       "        [ 0.8402, -1.5226, -0.7774, -0.9728,  0.2623,  1.9484, -1.1585,  1.8922,\n",
       "          0.7155,  0.1293],\n",
       "        [ 0.7430,  1.7675,  0.0258,  0.6285,  1.3089,  0.3253, -2.4253,  0.3428,\n",
       "         -1.0613,  1.7637],\n",
       "        [-0.1879, -0.9199,  1.0139,  2.8269,  1.6581, -1.7827,  0.8733,  1.2514,\n",
       "         -1.3100, -2.0500],\n",
       "        [-0.3762, -1.0989,  1.9507, -0.7418,  0.3374, -0.7515,  0.3272, -0.1313,\n",
       "         -0.8547,  1.1360]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.nn.Embedding(5, 10)\n",
    "l.weight = torch.nn.Parameter(layer.weight.get_plain_text())\n",
    "l(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 0., 1., 2., 3., 4.])\n",
      "<built-in method type of Tensor object at 0x38764ba70>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[11., 21., 31.],\n",
       "        [12., 22., 32.],\n",
       "        [10., 20., 30.],\n",
       "        [11., 21., 31.],\n",
       "        [12., 22., 32.],\n",
       "        [13., 23., 33.],\n",
       "        [14., 24., 34.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_enc = crypten.cryptensor(torch.tensor([1, 2, 0, 1, 2, 3, 4])) #, dtype=torch.long))\n",
    "print(data_enc.get_plain_text())\n",
    "lut = crypten.cryptensor(torch.tensor([[10, 20, 30], [11, 21, 31], [12, 22, 32], [13, 23, 33], [14, 24, 34]])).share\n",
    "# print(lut / 2**16)\n",
    "data_enc.evaluate_embed(lut).get_plain_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:inv_var.get_plain_text()=tensor([[3.8304, 3.5486, 4.0272],\n",
      "        [3.0654, 2.5956, 2.2561]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "forward\n",
      "output_enc\n",
      "output=tensor([[[ 1.2089,  4.1880,  2.9328, -1.1218],\n",
      "         [ 2.0143,  3.1603, -0.5563,  2.3644],\n",
      "         [ 1.4534,  1.7210,  5.9170, -1.1451]],\n",
      "\n",
      "        [[-0.0346,  3.8206,  5.3922,  1.3083],\n",
      "         [ 1.3207, -0.6203,  2.7214,  8.3293],\n",
      "         [ 0.7060,  4.9697,  1.1984,  1.6388]]])\n"
     ]
    }
   ],
   "source": [
    "model = nn.LayerNorm(4)\n",
    "model.weight = torch.tensor([1, 2, 3, 4])\n",
    "model.bias = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "model.encrypt(src=0)\n",
    "\n",
    "# Load data to Bob\n",
    "print('loading data')\n",
    "# data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=ALICE)\n",
    "data_enc = crypten.cryptensor(torch.rand(2, 3, 4)) #, dtype=torch.long))\n",
    "\n",
    "# print(f\"{data_enc.get_plain_text()=}\")\n",
    "# Classify the encrypted data\n",
    "model.eval()\n",
    "print(\"forward\")\n",
    "output_enc = model(data_enc)\n",
    "print('output_enc')\n",
    "# Compute the accuracy\n",
    "output = output_enc.get_plain_text()\n",
    "print(f\"{output=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2462,  4.5780,  2.9208, -2.0350],\n",
      "         [ 2.1834,  3.3537, -1.1494,  2.0914],\n",
      "         [ 1.5395,  1.6681,  6.4706, -2.1216]],\n",
      "\n",
      "        [[-0.1971,  4.1063,  5.7676,  0.8856],\n",
      "         [ 1.3703, -1.0258,  2.6783,  8.9994],\n",
      "         [ 0.6606,  5.4288,  0.9198,  1.2737]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[0.5748, 0.5381, 0.7149],\n",
      "        [0.4933, 0.5724, 0.3004]])\n",
      "tensor([[0.0654, 0.0778, 0.0581],\n",
      "        [0.1060, 0.1484, 0.1965]])\n",
      "tensor([[3.9090, 3.5858, 4.1501],\n",
      "        [3.0716, 2.5958, 2.2559]])\n"
     ]
    }
   ],
   "source": [
    "layer = torch.nn.LayerNorm(4)\n",
    "layer.weight = torch.nn.Parameter(torch.tensor([1.0, 2.0, 3.0, 4.0]))\n",
    "layer.bias = torch.nn.Parameter(torch.tensor([1.0, 2.0, 3.0, 4.0]))\n",
    "print(layer(data_enc.get_plain_text()))\n",
    "print(data_enc.get_plain_text().mean(dim=-1))\n",
    "print(data_enc.get_plain_text().var(dim=-1))\n",
    "print(1/data_enc.get_plain_text().var(dim=-1).sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_enc\n",
      "output=tensor([[[-0.3534, -1.6627, -0.4255,  ..., -2.0522, -0.3004,  2.2854],\n",
      "         [-0.3498, -1.6441, -0.4201,  ..., -2.0388, -0.2984,  2.2722],\n",
      "         [-0.3558, -1.6705, -0.4261,  ..., -2.0273, -0.2972,  2.2610],\n",
      "         ...,\n",
      "         [-0.3586, -1.6877, -0.4316,  ..., -2.0528, -0.3011,  2.2874],\n",
      "         [-0.3594, -1.6921, -0.4322,  ..., -2.0154, -0.2949,  2.2465],\n",
      "         [-0.3616, -1.6967, -0.4324,  ..., -2.0409, -0.2984,  2.2742]]])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Attention(768, 12)\n",
    "\n",
    "model.encrypt(src=0)\n",
    "\n",
    "# Load data to Bob\n",
    "print('loading data')\n",
    "# data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=ALICE)\n",
    "data_enc = crypten.cryptensor(torch.rand(1, 128, 768)) #, dtype=torch.long))\n",
    "\n",
    "# Classify the encrypted data\n",
    "model.eval()\n",
    "print(\"forward\")\n",
    "output_enc = model(data_enc)\n",
    "print('output_enc')\n",
    "# Compute the accuracy\n",
    "output = output_enc.get_plain_text()\n",
    "print(f\"{output=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_enc\n",
      "output=tensor([[[ 1.1848e+00,  3.3322e-01,  3.7883e-01,  ...,  6.9392e-01,\n",
      "           1.1551e+00, -2.1667e-02],\n",
      "         [ 5.4616e-01,  9.1588e-01, -1.9666e-01,  ...,  1.0253e+00,\n",
      "           5.2177e-01, -3.9018e-01],\n",
      "         [-7.3838e+07, -2.5166e+07,  9.3520e+07,  ..., -2.5163e+07,\n",
      "           1.7955e+08, -7.8470e+07],\n",
      "         ...,\n",
      "         [ 7.8235e-01,  1.1859e+00,  3.5832e-01,  ...,  8.9456e-01,\n",
      "           1.8242e-01,  4.4464e-01],\n",
      "         [ 8.2220e-01,  1.6330e-01,  4.1974e-01,  ...,  1.0208e+00,\n",
      "           9.9847e-01,  3.1905e-01],\n",
      "         [ 5.9824e-01,  6.0516e-02,  7.4852e-01,  ...,  1.6308e+00,\n",
      "           1.6758e+00, -5.7065e-01]]])\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Block, self).__init__()\n",
    "        embed_dim = embed_dim\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.Attention(embed_dim, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "model = Block(768, 12)\n",
    "model.encrypt(src=0)\n",
    "\n",
    "# Load data to Bob\n",
    "print('loading data')\n",
    "# data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=ALICE)\n",
    "data_enc = crypten.cryptensor(torch.rand(1, 128, 768)) #, dtype=torch.long))\n",
    "\n",
    "# Classify the encrypted data\n",
    "model.eval()\n",
    "print(\"forward\")\n",
    "output_enc = model(data_enc)\n",
    "print('output_enc')\n",
    "# Compute the accuracy\n",
    "output = output_enc.get_plain_text()\n",
    "print(f\"{output=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_enc\n",
      "output=tensor([[[ 3.5694e+09, -1.2558e+09, -7.0618e+08,  ..., -5.7081e+09,\n",
      "          -3.3800e+09,  7.3424e+08],\n",
      "         [ 3.5694e+09, -1.2558e+09, -7.0617e+08,  ..., -5.7081e+09,\n",
      "          -3.3800e+09,  7.3424e+08],\n",
      "         [ 3.5694e+09, -1.2558e+09, -7.0617e+08,  ..., -5.7081e+09,\n",
      "          -3.3800e+09,  7.3425e+08],\n",
      "         ...,\n",
      "         [ 3.5695e+09, -1.2556e+09, -7.0605e+08,  ..., -5.7080e+09,\n",
      "          -3.3799e+09,  7.3437e+08],\n",
      "         [ 3.5695e+09, -1.2556e+09, -7.0605e+08,  ..., -5.7079e+09,\n",
      "          -3.3799e+09,  7.3437e+08],\n",
      "         [ 3.5695e+09, -1.2556e+09, -7.0605e+08,  ..., -5.7079e+09,\n",
      "          -3.3799e+09,  7.3437e+08]]])\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_blocks, vocab_size, seq_len, full=True):\n",
    "        super(GPT, self).__init__()\n",
    "        self.full = full\n",
    "        if full:\n",
    "            self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "            self.pos_embed = crypten.cryptensor(torch.zeros(1, seq_len, embed_dim))\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(embed_dim, num_heads) for _ in range(num_blocks)]\n",
    "        )\n",
    "        if full:\n",
    "            self.ln = nn.LayerNorm(embed_dim)\n",
    "            self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "            self.softmax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        if self.full:\n",
    "            tok_embedding = self.tok_embed(x)\n",
    "            pos_embedding = self.pos_embed[:, :x.size()[1], :]\n",
    "            x = tok_embedding + pos_embedding\n",
    "        x = self.blocks(x)\n",
    "        if self.full:\n",
    "            x = self.ln(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "full = False\n",
    "# model = GPT(768, 12, 12, 50257, 128, full) # gpt2 13.5s\n",
    "model = GPT(2048, 16, 24, 50257, 128, full) # gpt-neo 2m 43.6s\n",
    "model.encrypt(src=0)\n",
    "\n",
    "# Load data to Bob\n",
    "print('loading data')\n",
    "# data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=ALICE)\n",
    "if full:\n",
    "    data_enc = crypten.cryptensor(torch.arange(64).reshape(1, 64))\n",
    "else:\n",
    "    data_enc = crypten.cryptensor(torch.arange(64 * 2048).reshape(1, 64, 2048))\n",
    "\n",
    "# Classify the encrypted data\n",
    "model.eval()\n",
    "print(\"forward\")\n",
    "output_enc = model(data_enc)\n",
    "print('output_enc')\n",
    "# Compute the accuracy\n",
    "output = output_enc.get_plain_text()\n",
    "print(f\"{output=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear Init\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In LayerNorm\n",
      "INFO:root:==================\n",
      "INFO:root:weight=Parameter containing:\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n",
      "INFO:root:bias=Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In AUTOGRAD\n",
      "INFO:root:==================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_enc\n",
      "output=tensor([[[-5.5166e+08, -1.0820e+08,  8.9591e+08,  ..., -1.1512e+09,\n",
      "           3.2500e+08,  8.6850e+08],\n",
      "         [-5.5166e+08, -1.0820e+08,  8.9591e+08,  ..., -1.1512e+09,\n",
      "           3.2500e+08,  8.6850e+08],\n",
      "         [-5.5166e+08, -1.0820e+08,  8.9591e+08,  ..., -1.1512e+09,\n",
      "           3.2500e+08,  8.6850e+08],\n",
      "         ...,\n",
      "         [-5.5166e+08, -1.0820e+08,  8.9591e+08,  ..., -1.1512e+09,\n",
      "           3.2500e+08,  8.6850e+08],\n",
      "         [-5.5166e+08, -1.0820e+08,  8.9591e+08,  ..., -1.1512e+09,\n",
      "           3.2500e+08,  8.6850e+08],\n",
      "         [-5.5166e+08, -1.0820e+08,  8.9591e+08,  ..., -1.1512e+09,\n",
      "           3.2500e+08,  8.6850e+08]]])\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(BertBlock, self).__init__()\n",
    "        embed_dim = embed_dim\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.Attention(embed_dim, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x + self.attn(x))\n",
    "        x = self.ln2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_blocks, vocab_size, seq_len, full=True):\n",
    "        super(Bert, self).__init__()\n",
    "        self.full = full\n",
    "        if full:\n",
    "            self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "            self.pos_embed = crypten.cryptensor(torch.zeros(1, seq_len, embed_dim))\n",
    "        self.ln = nn.LayerNorm\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[BertBlock(embed_dim, num_heads) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        if full:\n",
    "            self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "            self.softmax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        if self.full:\n",
    "            tok_embedding = self.tok_embed(x)\n",
    "            pos_embedding = self.pos_embed[:, :x.size()[1], :]\n",
    "            x = tok_embedding + pos_embedding\n",
    "        x = self.ln(x)\n",
    "        x = self.blocks(x)\n",
    "        if self.full:\n",
    "            x = self.fc(x)\n",
    "            x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "full = False\n",
    "# model = Bert(128, 2, 2, 30522, 128, full) # bert tiny 0.3s\n",
    "# model = Bert(768, 12, 12, 30522, 128, full) # bert base 13.5s\n",
    "model = Bert(1024, 16, 24, 30522, 128, full) # bert large 44.8s\n",
    "model.encrypt(src=0)\n",
    "\n",
    "# Load data to Bob\n",
    "print('loading data')\n",
    "# data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=ALICE)\n",
    "if full:\n",
    "    data_enc = crypten.cryptensor(torch.arange(64).reshape(1, 64))\n",
    "else:\n",
    "    data_enc = crypten.cryptensor(torch.arange(64 * 1024).reshape(1, 64, 1024))\n",
    "\n",
    "# Classify the encrypted data\n",
    "model.eval()\n",
    "print(\"forward\")\n",
    "output_enc = model(data_enc)\n",
    "print('output_enc')\n",
    "# Compute the accuracy\n",
    "output = output_enc.get_plain_text()\n",
    "print(f\"{output=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m         logging\u001b[38;5;241m.\u001b[39mgetLogger()\u001b[38;5;241m.\u001b[39msetLevel(level)\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[0;32m---> 57\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39mencrypt(src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Load data to Bob\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mAttention.__init__\u001b[0;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embed_dim \u001b[38;5;241m%\u001b[39m num_heads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid heads and embedding dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_dim \u001b[38;5;241m=\u001b[39m embed_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_heads\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"invalid heads and embedding dimension\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = config.num_heads\n",
    "        self.search_dim = embed_dim // num_heads\n",
    "\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        level = logging.getLogger().level\n",
    "        logging.getLogger().setLevel(logging.INFO)\n",
    "        logging.info(\"==================\")\n",
    "        logging.info(\"In forward\" )\n",
    "        logging.info(\"==================\")\n",
    "\n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).transpose(1, 2)\n",
    "\n",
    "        logging.info(f\"{q.shape=}\")\n",
    "\n",
    "        attn = q.matmul(k_t) / math.sqrt(q.size(-1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        logging.info(f\"{attn.shape=}\")\n",
    "        logging.info(f\"{v.shape=}\")\n",
    "\n",
    "        y = attn.matmul(v)\n",
    "\n",
    "        logging.info(f\"{y.shape=}\")\n",
    "\n",
    "        y = y.transpose(1, 2)\n",
    "\n",
    "        logging.info(f\"{y.shape=}\")\n",
    "\n",
    "        y = y.reshape(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        logging.info(f\"{y.shape=}\")\n",
    "\n",
    "        logging.getLogger().setLevel(level)\n",
    "\n",
    "        return y\n",
    "\n",
    "model = Attention(768, 12)\n",
    "\n",
    "model.encrypt(src=0)\n",
    "\n",
    "# Load data to Bob\n",
    "print('loading data')\n",
    "# data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=ALICE)\n",
    "data_enc = crypten.cryptensor(torch.rand(1, 128, 768)) #, dtype=torch.long))\n",
    "\n",
    "# Classify the encrypted data\n",
    "model.eval()\n",
    "print(\"forward\")\n",
    "output_enc = model(data_enc)\n",
    "print('output_enc')\n",
    "# Compute the accuracy\n",
    "output = output_enc.get_plain_text()\n",
    "print(f\"{output=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define the structure of Alice's network as a class. Even though Alice has a pre-trained model, the CrypTen will require this structure as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import crypten.nn as nn\n",
    "\n",
    "class GPTConfig:\n",
    "    # Set dropout to 0 for inference. It is only needed for training\n",
    "    attn_dropout = 0.1\n",
    "    embed_dropout = 0.1\n",
    "    ff_dropout = 0.1\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, max_len, **kwargs\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    num_heads = 12\n",
    "    num_blocks = 12\n",
    "    embed_dim = 768\n",
    "\n",
    "vocab_size = 50257\n",
    "max_len = 1024\n",
    "\n",
    "config = GPT1Config(vocab_size, max_len)\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "#     def __init__(self, ndim, bias):\n",
    "#         super(LayerNorm, self).__init__()\n",
    "#         self.weight = nn.Parameter(torch.ones(ndim))\n",
    "#         self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        embed_dim = config.embed_dim\n",
    "        self.num_heads = config.num_heads\n",
    "        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n",
    "\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "        self.proj_dropout = nn.Dropout(config.ff_dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(config.max_len, config.max_len))\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        with open(\"foo.txt\", \"w\") as file:\n",
    "            file.write(\"Your text goes here\")\n",
    "\n",
    "        level = logging.getLogger().level\n",
    "        logging.getLogger().setLevel(logging.INFO)\n",
    "        logging.info(\"==================\")\n",
    "        logging.info(\"In forward\" )\n",
    "        logging.info(\"==================\")\n",
    "\n",
    "        # x.shape == (batch_size, seq_len, embed_dim)\n",
    "        k_t = self.key(x).T\n",
    "        v = self.value(x)\n",
    "        q = self.query(x)\n",
    "        # shape == (batch_size, num_heads, seq_len, head_dim)\n",
    "        logging.info(f\"{k_t.shape=}\")\n",
    "        logging.info(f\"{v.shape=}\")\n",
    "        logging.info(f\"{q.shape=}\")\n",
    "\n",
    "        logging.info(\"KQV created\")\n",
    "        attn = torch.matmul(q, k_t) / torch.sqrt(q.size(-1))\n",
    "\n",
    "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
    "        # attn = attn.masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "        logging.info(\"masked fill\")\n",
    "\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        logging.info('here %s %s', attn.shape, v.shape)\n",
    "\n",
    "        y = torch.matmul(attn, v)\n",
    "        logging.info(\"matmul done\")\n",
    "\n",
    "        # y.shape == (batch_size, seq_len, embed_dim)\n",
    "        y = self.proj_dropout(self.proj(y))\n",
    "        logging.info(\"proj_dropout\")\n",
    "        logging.info(f\"{y.type}\")\n",
    "\n",
    "        logging.getLogger().setLevel(level)\n",
    "\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.ln1 = nn.BatchNorm1d(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiheadAttention(config)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(config.ff_dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.embed_dim\n",
    "        self.max_len = config.max_len\n",
    "        # self.tok_embed = nn.Embedding(\n",
    "        #     config.vocab_size, embed_dim\n",
    "        # )\n",
    "        # self.pos_embed = nn.Parameter(\n",
    "        #     torch.zeros(1, config.max_len, embed_dim)\n",
    "        # )\n",
    "        self.dropout = nn.Dropout(config.embed_dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.num_blocks)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, config.vocab_size)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        # embed_dim = config.embed_dim\n",
    "        # batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        assert seq_len <= self.max_len, \"sequence longer than model capacity\"\n",
    "\n",
    "        # tok_embedding = self.tok_embed(x)\n",
    "        # tok_embedding.shape == (batch_size, seq_len, embed_dim)\n",
    "        # pos_embedding = self.pos_embed[:, :seq_len, :]\n",
    "        # pos_embedding.shape == (1, seq_len, embed_dim)\n",
    "        # x = self.dropout(tok_embedding + pos_embedding)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        # x.shape == (batch_size, seq_len, vocab_size)\n",
    "        return x\n",
    "\n",
    "model = GPT(config)\n",
    "# model = Block(config)\n",
    "# model = MultiheadAttention(config)\n",
    "# model = LayerNorm(ndim=10, bias=1)\n",
    "\n",
    "crypten.common.serial.register_safe_class(GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a helper routine `compute_accuracy` to make it easy to compute the accuracy of the output we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(output, labels):\n",
    "    pred = output.argmax(1)\n",
    "    correct = pred.eq(labels)\n",
    "    correct_count = correct.sum(0, keepdim=True).float()\n",
    "    accuracy = correct_count.mul_(100.0 / output.size(0))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypting a Pre-trained Model\n",
    "\n",
    "Assume that Alice has a pre-trained network ready to classify data. Let's see how we can use CrypTen to encrypt this network, so it can be used to classify data without revealing its parameters. We'll use the pre-trained model in `models/tutorial4_alice_model.pth` in this tutorial. As in Tutorial 3, we will assume Alice is using the rank 0 process, while Bob is using the rank 1 process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALICE = 0\n",
    "BOB = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CrypTen, encrypting PyTorch network is straightforward: we load a PyTorch model from file to the appropriate source, convert it to a CrypTen model and then encrypt it. Let us understand each of these steps.\n",
    "\n",
    "As we did with CrypTensors in Tutorial 3, we will use CrypTen's load functionality (i.e., `crypten.load`) to read a model from file to a particular source. The source is indicated by the keyword argument `src`. As in Tutorial 3, this src argument tells us the rank of the party we want to load the model to (and later, encrypt the model from). In addition, here we also need to provide a dummy model to tell CrypTen the model's structure. The dummy model is indicated by the keyword argument `dummy_model`. Note that unlike loading a tensor, the result from `crypten.load` is not encrypted. Instead, only the `src` party's model is populated from the file.\n",
    "\n",
    "Once the model is loaded, we call the function `from_pytorch`: this function sets up a CrypTen network from the PyTorch network. It takes the plaintext network as input as well as dummy input. The dummy input must be a `torch` tensor of the same shape as a potential input to the network, however the values inside the tensor do not matter.  \n",
    "\n",
    "Finally, we call `encrypt` on the CrypTen network to encrypt its parameters. Once we call the `encrypt` function, the models `encrypted` property will verify that the model parameters have been encrypted. (Encrypted CrypTen networks can also be decrypted using the `decrypt` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model to Alice\n",
    "# dummy_model = AliceNet()\n",
    "# plaintext_model = torch.load('models/gpt2.bin')\n",
    "# model.load_state_dict(plaintext_model)\n",
    "plaintext_model = model\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Encrypt the model from Alice:\n",
    "\n",
    "# 1. Create a dummy input with the same shape as the model input\n",
    "dummy_input = torch.empty((1024, 768), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 2. Construct a CrypTen network with the trained model and dummy_input\n",
    "private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "\n",
    "# 3. Encrypt the CrypTen network with src=ALICE\n",
    "private_model.encrypt(src=ALICE)\n",
    "\n",
    "#Check that model is encrypted:\n",
    "print(\"Model successfully encrypted:\", private_model.encrypted)\n",
    "\n",
    "print(private_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Encrypted Data with Encrypted Model\n",
    "\n",
    "We can now use Alice's encrypted network to classify Bob's data. For this, we need to encrypt Bob's data as well, as we did in Tutorial 3 (recall that Bob has the rank 1 process). Once Alice's network and Bob's data are both encrypted, CrypTen inference is performed with essentially identical steps as in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "# model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.set_start_method('fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==================\n",
      "INFO:root:In forward\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:==================\n",
      "INFO:root:In Linear\n",
      "INFO:root:==================\n",
      "INFO:root:q=MPCTensor(\n",
      "\t_tensor=tensor([[-10347,  -2793,  -4438,  ...,   -382, -23082,  -2037],\n",
      "        [-11186,   8446,   5831,  ...,   5702,  -8415,  -9827],\n",
      "        [-11965, -16345,  -1484,  ..., -23496, -13105, -17982],\n",
      "        ...,\n",
      "        [  7657, -12255,   9758,  ...,   2460, -18667, -30977],\n",
      "        [  6498, -17621,  18986,  ...,   4994, -34420, -21247],\n",
      "        [ -5863,  -8031,  29161,  ...,   7533, -24777,  -2178]])\n",
      "\tplain_text=HIDDEN\n",
      "\tptype=ptype.arithmetic\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "started\n",
      "loading\n",
      "loaded gpt2\n",
      "dummy_input\n",
      "encrypting\n",
      "loading data\n",
      "flattened\n",
      "forward\n",
      "output_enc\n",
      "\tAccuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "print(\"starting\")\n",
    "labels = torch.load('/tmp/bob_test_labels.pth').long()\n",
    "count = 100 # For illustration purposes, we'll use only 100 samples for classification\n",
    "print(\"started\")\n",
    "\n",
    "# @mpc.run_multiprocess(world_size=2)\n",
    "def encrypt_model_and_data():\n",
    "    print(\"loading\")\n",
    "    # Load pre-trained model to Alice\n",
    "    # model = crypten.load_from_party('models/gpt2.bin', src=ALICE)\n",
    "    print(\"loaded gpt2\")\n",
    "    # Encrypt model from Alice\n",
    "    dummy_input = torch.empty((1024, 768)) #, dtype=torch.long)\n",
    "    print('dummy_input')\n",
    "    # private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "    private_model = model\n",
    "    print(\"encrypting\")\n",
    "    private_model.encrypt(src=ALICE)\n",
    "\n",
    "    # Load data to Bob\n",
    "    print('loading data')\n",
    "    # data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=ALICE)\n",
    "    data_enc = crypten.cryptensor(torch.rand(1024, 768)) #, dtype=torch.long))\n",
    "    data_enc2 = data_enc[:count]\n",
    "    data_flatten = data_enc2.flatten(start_dim=1)\n",
    "    print('flattened')\n",
    "\n",
    "    # Classify the encrypted data\n",
    "    private_model.eval()\n",
    "    print(\"forward\")\n",
    "    output_enc = private_model(data_flatten)\n",
    "    print('output_enc')\n",
    "    # Compute the accuracy\n",
    "    output = output_enc.get_plain_text()\n",
    "    accuracy = compute_accuracy(output, labels[:count])\n",
    "    crypten.print(\"\\tAccuracy: {0:.4f}\".format(accuracy.item()))\n",
    "\n",
    "encrypt_model_and_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Encrypted Classification\n",
    "\n",
    "Finally, we will verify that CrypTen classification results in encrypted output, and that this output can be decrypted into meaningful labels. \n",
    "\n",
    "To see this, in this tutorial, we will just check whether the result is an encrypted tensor; in the next tutorial, we will look into the values of tensor and confirm the encryption. We will also decrypt the result. As we discussed before, Alice and Bob both have access to the decrypted output of the model, and can both use this to obtain the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/memo/Documents/curl/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/__/frht1s0n5hd1nnltt_cl9m1r0000gn/T/ipykernel_58278/2535009032.py\", line 4, in encrypt_model_and_data\n",
      "    plaintext_model = crypten.load_from_party('models/tutorial4_alice_model.pth', src=ALICE)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/memo/Documents/curl/crypten/__init__.py\", line 337, in load_from_party\n",
      "    result = load_closure(f, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/serialization.py\", line 1040, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/serialization.py\", line 1268, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/serialization.py\", line 1073, in find_class\n",
      "    return super().find_class(mod_name, name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'AliceNet' on <module '__main__'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     crypten\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecrypted labels:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, pred)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mencrypt_model_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/curl/crypten/mpc/context.py:97\u001b[0m, in \u001b[0;36mrun_multiprocess.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     process\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[0;32m---> 97\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m was_initialized:\n\u001b[1;32m    100\u001b[0m     crypten\u001b[38;5;241m.\u001b[39minit()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/memo/Documents/curl/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/__/frht1s0n5hd1nnltt_cl9m1r0000gn/T/ipykernel_58278/2535009032.py\", line 4, in encrypt_model_and_data\n",
      "    plaintext_model = crypten.load_from_party('models/tutorial4_alice_model.pth', src=ALICE)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/memo/Documents/curl/crypten/__init__.py\", line 356, in load_from_party\n",
      "    result = comm.get().broadcast_obj(None, src)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/memo/Documents/curl/crypten/communicator/communicator.py\", line 201, in logging_wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/memo/Documents/curl/crypten/communicator/distributed_communicator.py\", line 317, in broadcast_obj\n",
      "    dist.broadcast(size, src, group=group)\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 72, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/memo/.pyenv/versions/3.11.4/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 1918, in broadcast\n",
      "    work.wait()\n",
      "RuntimeError: [/Users/runner/work/pytorch/pytorch/pytorch/third_party/gloo/gloo/transport/uv/unbound_buffer.cc:67] Timed out waiting 1800000ms for recv operation to complete\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def encrypt_model_and_data():\n",
    "    # Load pre-trained model to Alice\n",
    "    plaintext_model = crypten.load_from_party('models/tutorial4_alice_model.pth', src=ALICE)\n",
    "\n",
    "    # Encrypt model from Alice\n",
    "    dummy_input = torch.empty((1, 784))\n",
    "    private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "    private_model.encrypt(src=ALICE)\n",
    "\n",
    "    # Load data to Bob\n",
    "    data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=BOB)\n",
    "    data_enc2 = data_enc[:count]\n",
    "    data_flatten = data_enc2.flatten(start_dim=1)\n",
    "\n",
    "    # Classify the encrypted data\n",
    "    private_model.eval()\n",
    "    output_enc = private_model(data_flatten)\n",
    "\n",
    "    # Verify the results are encrypted:\n",
    "    crypten.print(\"Output tensor encrypted:\", crypten.is_encrypted_tensor(output_enc))\n",
    "\n",
    "    # Decrypting the result\n",
    "    output = output_enc.get_plain_text()\n",
    "\n",
    "    # Obtaining the labels\n",
    "    pred = output.argmax(dim=1)\n",
    "    crypten.print(\"Decrypted labels:\\n\", pred)\n",
    "\n",
    "encrypt_model_and_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This completes our tutorial. While we have used a simple network here to illustrate the concepts, CrypTen provides primitives to allow for encryption of substantially more complex networks. In our examples section, we demonstrate how CrypTen can be used to encrypt LeNet and ResNet, among others. \n",
    "\n",
    "Before exiting this tutorial, please clean up the files generated using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filenames = ['/tmp/alice_train.pth',\n",
    "             '/tmp/alice_train_labels.pth',\n",
    "             '/tmp/bob_test.pth',\n",
    "             '/tmp/bob_test_labels.pth']\n",
    "\n",
    "for fn in filenames:\n",
    "    if os.path.exists(fn): os.remove(fn)"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "390894444956881"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "139932",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "375902760006757",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 4 -- Classification with Encrypted Neural Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
