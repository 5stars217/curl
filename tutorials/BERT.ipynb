{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight: torch.Size([28996, 768])\n",
      "bert.embeddings.position_embeddings.weight: torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight: torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight: torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.pooler.dense.weight: torch.Size([768, 768])\n",
      "bert.pooler.dense.bias: torch.Size([768])\n",
      "classifier.weight: torch.Size([2, 768])\n",
      "classifier.bias: torch.Size([2])\n",
      "n_weight=108208896, n_bias=102914, n_param=108311810\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained('gchhablani/bert-base-cased-finetuned-qnli')\n",
    "bert_model.eval()\n",
    "\n",
    "# Access the model's weights\n",
    "weights = bert_model.state_dict()\n",
    "\n",
    "# Modify the weights or perform any operation you desire\n",
    "# Example: Print the shape of each weight tensor\n",
    "o = 0\n",
    "b = 0\n",
    "for name, weight in weights.items():\n",
    "    if \"weight\" in str(name):\n",
    "        print(f\"{name}: {weight.size()}\")\n",
    "        p = 1\n",
    "        for w in weight.size():\n",
    "            p *= w\n",
    "        o += p\n",
    "    elif \"bias\" in str(name):\n",
    "        print(f\"{name}: {weight.size()}\")\n",
    "        p = 1\n",
    "        for w in weight.size():\n",
    "            p *= w\n",
    "        b += p\n",
    "    else:\n",
    "        print(f\"else {name}: {weight.size()}\")\n",
    "print(f\"n_weight={o}, n_bias={b}, n_param={o+b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.5781,  0.1473]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gchhablani/bert-base-cased-finetuned-qnli\")\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = bert_model(**inputs)\n",
    "realput = outputs\n",
    "print(realput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def load_tsv(data_file, max_seq_len, delimiter='\\t'):\n",
    "    '''Load a tsv '''\n",
    "    sentences = []\n",
    "    targets = []\n",
    "    with codecs.open(data_file, 'r', 'utf-8') as data_fh:\n",
    "        for _ in range(1):\n",
    "            data_fh.readline()\n",
    "        for row in data_fh:\n",
    "            row = row.strip().split(delimiter)\n",
    "            sentences.append(tokenizer(row[1], row[2], return_tensors=\"pt\"))\n",
    "            targets.append(1*(row[3] == \"not_entailment\"))\n",
    "    return sentences, targets\n",
    "\n",
    "data, targets = load_tsv(\"GLUE-baselines/glue_data/QNLI/dev.tsv\", 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for label in range(100):\n",
    "    outputs = bert_model(**data[label])\n",
    "    count += targets[label] == outputs.logits.argmax()\n",
    "    total += 1\n",
    "count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super(Attention, self).__init__()\n",
    "\n",
    "#         assert embed_dim % num_heads == 0, \"invalid heads and embedding dimension\"\n",
    "\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.search_dim = embed_dim // num_heads\n",
    "\n",
    "#         self.key = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.value = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.query = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.shape[0]\n",
    "#         seq_len = x.shape[1]\n",
    "\n",
    "#         k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).permute(0, 2, 3, 1)\n",
    "#         v = self.value(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).transpose(1, 2)\n",
    "#         q = self.query(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).transpose(1, 2)\n",
    "\n",
    "#         attn = q.matmul(k_t) / math.sqrt(q.size(-1))\n",
    "#         attn = attn.softmax(dim=-1)\n",
    "#         y = attn.matmul(v)\n",
    "#         y = y.transpose(1, 2)\n",
    "#         y = y.reshape(batch_size, seq_len, self.embed_dim)\n",
    "#         return y\n",
    "\n",
    "# class BertBlock(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super(BertBlock, self).__init__()\n",
    "#         embed_dim = embed_dim\n",
    "#         self.ln1 = nn.LayerNorm(embed_dim)\n",
    "#         self.ln2 = nn.LayerNorm(embed_dim)\n",
    "#         self.attention = Attention(embed_dim, num_heads)\n",
    "#         self.ff = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, embed_dim * 4),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(embed_dim * 4, embed_dim),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.ln1(x + self.attn(x))\n",
    "#         x = self.ln2(x + self.ff(x))\n",
    "#         return x\n",
    "\n",
    "# class Bert(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, num_blocks, vocab_size, seq_len):\n",
    "#         super(Bert, self).__init__()\n",
    "#         self.embeddings = BertEmbeddings(vocab_size, embed_dim, seq_len)\n",
    "#         self.encoder = nn.ModuleList(\n",
    "#             [BertBlock(embed_dim, num_heads) for _ in range(num_blocks)]\n",
    "#         )\n",
    "#         self.pooler = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.classifier = nn.Linear(embed_dim, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.full:\n",
    "#             word_embedding = self.word_embed(x[\"input_ids\"])\n",
    "#             pos_embedding = self.pos_embed[:, :x[\"input_ids\"].size()[1], :]\n",
    "#             type_embedding = self.type_embed(x[\"token_type_ids\"])\n",
    "#             x = word_embedding + pos_embedding + type_embedding\n",
    "#             x = self.ln(x)\n",
    "#         x = self.blocks(x)\n",
    "#         if self.full:\n",
    "#             x = self.pooler(x[:, 0, :])\n",
    "#             x = x.tanh()\n",
    "#             x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "# class LLM:\n",
    "#     def __init__(self, model):\n",
    "#         self.bert = model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.bert(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, emb_size)\n",
    "        self.token_type_embeddings = nn.Embedding(2, emb_size)\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        word_emb = self.word_embeddings(input_ids)\n",
    "        pos_emb = self.position_embeddings.weight[:input_ids.size()[1], :]\n",
    "        type_emb = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        emb = word_emb + pos_emb + type_emb\n",
    "        emb = self.LayerNorm(emb)\n",
    "\n",
    "        return emb\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = emb_size // self.n_heads\n",
    "        self.query = nn.Linear(emb_size, emb_size)\n",
    "        self.key = nn.Linear(emb_size, emb_size)\n",
    "        self.value = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, emb):\n",
    "        B, T, C = emb.shape  # batch size, sequence length, embedding size\n",
    "\n",
    "        q = self.query(emb).view(B, T, self.n_heads, self.head_size).transpose(1, 2)\n",
    "        k = self.key(emb).view(B, T, self.n_heads, self.head_size).transpose(1, 2)\n",
    "        v = self.value(emb).view(B, T, self.n_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * self.head_size**-0.5\n",
    "\n",
    "        weights = weights.softmax(dim=-1)\n",
    "\n",
    "        emb_rich = weights @ v\n",
    "        emb_rich = emb_rich.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return emb_rich\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(emb_size, emb_size)\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, emb_rich, emb):\n",
    "        x = self.dense(emb_rich)\n",
    "        x = x + emb\n",
    "        out = self.LayerNorm(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(emb_size, n_heads)\n",
    "        self.output = BertSelfOutput(emb_size)\n",
    "\n",
    "    def forward(self, emb):\n",
    "        emb_rich = self.self(emb)\n",
    "        out = self.output(emb_rich, emb)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(emb_size, 4 * emb_size)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, att_out):\n",
    "        x = self.dense(att_out)\n",
    "        out = self.gelu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(4 * emb_size, emb_size)\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, intermediate_out, att_out):\n",
    "        x = self.dense(intermediate_out)\n",
    "        x = x + att_out\n",
    "        out = self.LayerNorm(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads ):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(emb_size, n_heads)\n",
    "        self.intermediate = BertIntermediate(emb_size)\n",
    "        self.output = BertOutput(emb_size)\n",
    "\n",
    "    def forward(self, emb):\n",
    "        att_out = self.attention(emb)\n",
    "        intermediate_out = self.intermediate(att_out)\n",
    "        out = self.output(intermediate_out, att_out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(emb_size, n_heads) for i in range(12)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layer:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(emb_size, emb_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, encoder_out):\n",
    "        pool_first_token = encoder_out[:, 0]\n",
    "        out = self.dense(pool_first_token)\n",
    "        out = self.tanh(out)\n",
    "        return out\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, seq_len, n_heads):\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings(vocab_size, emb_size, seq_len)\n",
    "        self.encoder = BertEncoder(emb_size, n_heads)\n",
    "        self.pooler = BertPooler(emb_size)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        emb = self.embeddings(input_ids, token_type_ids)\n",
    "        out = self.encoder(emb)\n",
    "        pooled_out = self.pooler(out)\n",
    "        return out, pooled_out\n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, seq_len, n_heads):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel(vocab_size, emb_size, seq_len, n_heads)\n",
    "        self.classifier = nn.Linear(emb_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        _, pooled_out = self.bert(input_ids, token_type_ids)\n",
    "        logits = self.classifier(pooled_out)\n",
    "        return logits\n",
    "\n",
    "# plain_model = LLM(Bert(768, 12, 12, 30522, 1024))\n",
    "plain_model = BertForSequenceClassification(28996, 768, 512, 12)\n",
    "plain_model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_model.bert.embeddings.word_embeddings.weight = nn.Parameter(weights[\"bert.embeddings.word_embeddings.weight\"])\n",
    "plain_model.bert.embeddings.position_embeddings.weight = nn.Parameter(weights[\"bert.embeddings.position_embeddings.weight\"][None, :, :])\n",
    "plain_model.bert.embeddings.token_type_embeddings.weight = nn.Parameter(weights[\"bert.embeddings.token_type_embeddings.weight\"])\n",
    "plain_model.bert.embeddings.LayerNorm.weight = nn.Parameter(weights[\"bert.embeddings.LayerNorm.weight\"])\n",
    "plain_model.bert.embeddings.LayerNorm.bias = nn.Parameter(weights[\"bert.embeddings.LayerNorm.bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load_state_dict\n",
    "for m in range(len(plain_model.blocks._modules)):\n",
    "    layer = \"bert.encoder.layer.\"\n",
    "    plain_model.blocks._modules[str(m)].attn.query.weight = nn.Parameter(weights[layer+str(m)+\".attention.self.query.weight\"])\n",
    "    plain_model.blocks._modules[str(m)].attn.query.bias = nn.Parameter(weights[layer+str(m)+\".attention.self.query.bias\"])\n",
    "    plain_model.blocks._modules[str(m)].attn.key.weight = nn.Parameter(weights[layer+str(m)+\".attention.self.key.weight\"])\n",
    "    plain_model.blocks._modules[str(m)].attn.key.bias = nn.Parameter(weights[layer+str(m)+\".attention.self.key.bias\"])\n",
    "    plain_model.blocks._modules[str(m)].attn.value.weight = nn.Parameter(weights[layer+str(m)+\".attention.self.value.weight\"])\n",
    "    plain_model.blocks._modules[str(m)].attn.value.bias = nn.Parameter(weights[layer+str(m)+\".attention.self.value.bias\"])\n",
    "    plain_model.blocks._modules[str(m)].attn.proj.weight = nn.Parameter(weights[layer+str(m)+\".attention.output.dense.weight\"]) # .t()\n",
    "    plain_model.blocks._modules[str(m)].attn.proj.bias = nn.Parameter(weights[layer+str(m)+\".attention.output.dense.bias\"])\n",
    "    plain_model.blocks._modules[str(m)].ln1.weight = nn.Parameter(weights[layer+str(m)+\".attention.output.LayerNorm.weight\"])\n",
    "    plain_model.blocks._modules[str(m)].ln1.bias = nn.Parameter(weights[layer+str(m)+\".attention.output.LayerNorm.bias\"])\n",
    "    plain_model.blocks._modules[str(m)].ff._modules['0'].weight = nn.Parameter(weights[layer+str(m)+\".intermediate.dense.weight\"]) # .t()\n",
    "    plain_model.blocks._modules[str(m)].ff._modules['0'].bias = nn.Parameter(weights[layer+str(m)+\".intermediate.dense.bias\"])\n",
    "    plain_model.blocks._modules[str(m)].ff._modules['2'].weight = nn.Parameter(weights[layer+str(m)+\".output.dense.weight\"]) # .t()\n",
    "    plain_model.blocks._modules[str(m)].ff._modules['2'].bias = nn.Parameter(weights[layer+str(m)+\".output.dense.bias\"])\n",
    "    plain_model.blocks._modules[str(m)].ln2.weight = nn.Parameter(weights[layer+str(m)+\".output.LayerNorm.weight\"])\n",
    "    plain_model.blocks._modules[str(m)].ln2.bias = nn.Parameter(weights[layer+str(m)+\".output.LayerNorm.bias\"])\n",
    "plain_model.pooler.weight = nn.Parameter(weights[\"bert.pooler.dense.weight\"]) # .t()\n",
    "plain_model.pooler.bias = nn.Parameter(weights[\"bert.pooler.dense.bias\"])\n",
    "plain_model.classifier.weight = nn.Parameter(weights[\"classifier.weight\"]) # .t()\n",
    "plain_model.classifier.bias = nn.Parameter(weights[\"classifier.bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "forward\n",
      "output\n",
      "output.shape=torch.Size([1, 2])\n",
      "output=tensor([[-0.7974,  0.5348]], grad_fn=<AddmmBackward0>)\n",
      "0.5348405241966248 -0.7973539233207703\n",
      "-0.13125669956207275 0.8873710036277771\n"
     ]
    }
   ],
   "source": [
    "# Load data to Bob\n",
    "print('loading data')\n",
    "# Classify the encrypted data\n",
    "print(\"forward\")\n",
    "x = {}\n",
    "x['input_ids'] = data[0][\"input_ids\"]\n",
    "x['token_type_ids'] = data[0][\"token_type_ids\"]\n",
    "# x['input_ids'] = torch.tensor([1])\n",
    "# x['token_type_ids'] = torch.tensor([0, 1, 0, 1])\n",
    "word_embedding = plain_model.word_embed(x[\"input_ids\"])\n",
    "pos_embedding = plain_model.pos_embed[:, :x[\"input_ids\"].size()[1], :]\n",
    "type_embedding = plain_model.type_embed(x[\"token_type_ids\"])\n",
    "x = word_embedding + pos_embedding + type_embedding\n",
    "x = plain_model.ln(x)\n",
    "# v = '0'\n",
    "# x = plain_model.blocks._modules[v].attn(x)\n",
    "x = plain_model.blocks._modules['0'](x)\n",
    "x = plain_model.blocks._modules['1'](x)\n",
    "x = plain_model.blocks._modules['2'](x)\n",
    "x = plain_model.blocks._modules['3'](x)\n",
    "x = plain_model.blocks._modules['4'](x)\n",
    "x = plain_model.blocks._modules['5'](x)\n",
    "x = plain_model.blocks._modules['6'](x)\n",
    "x = plain_model.blocks._modules['7'](x)\n",
    "x = plain_model.blocks._modules['8'](x)\n",
    "x = plain_model.blocks._modules['9'](x)\n",
    "x = plain_model.blocks._modules['10'](x)\n",
    "x = plain_model.blocks._modules['11'](x)\n",
    "x = plain_model.pooler(x[:, 0, :])\n",
    "x = x.tanh()\n",
    "x = plain_model.classifier(x)\n",
    "\n",
    "# x = plain_model(x)\n",
    "print('output')\n",
    "# Compute the accuracy\n",
    "output = x\n",
    "print(f\"{output.shape=}\")\n",
    "print(f\"{output=}\")\n",
    "print(f\"{output.max()} {output.min()}\")\n",
    "print(f\"{output.mean()} {output.var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9990, -1.6248]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5682,  2.9418]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.8683,  2.4348]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1733, -2.6358]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2052,  1.8399]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6817,  3.0015]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3191,  1.2331]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7189,  2.9994]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8530,  3.0773]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5190, -2.9471]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6168,  2.9147]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6551, -2.9953]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3681, -2.8038]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8698,  3.0741]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7479,  1.3016]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.7329, -3.1091]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3685, -2.8042]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1636, -2.6641]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5054, -2.9156]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.9104,  3.0573]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4702, -2.8864]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7427,  3.0555]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.5104, -2.0535]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.0512, -2.6006]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6803, -3.0739]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6611, -3.1033]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.1912,  2.5616]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.0222, -2.5347]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4024, -2.8362]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.0406, -2.5552]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5356, -2.9468]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3501, -2.8404]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7789,  3.0134]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.6371,  2.2064]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.9372,  3.1118]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.5423,  2.0899]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.7349, -3.1063]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.8594, -2.4279]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.7393, -3.0053]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.7682, -3.0667]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.4013,  2.7568]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4583, -2.9233]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7767,  3.0283]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1787,  1.8294]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8773,  3.1095]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7066,  3.0171]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1982,  0.2772]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.0149,  2.6298]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8305,  3.0703]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.6108, -2.2148]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7584,  3.0453]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4992, -2.8787]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.4944, -1.0677]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8817,  3.0804]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.4016, -1.9658]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6998,  2.9333]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1238,  0.0610]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.8582, -2.4070]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.2865, -2.7253]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3104, -2.8145]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4477,  1.2743]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5070, -2.9099]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8925,  3.0805]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4997, -2.8727]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6377, -3.0012]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8546,  3.0752]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.2514,  2.6694]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5464,  1.3610]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6890,  2.9711]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1273, -2.6294]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6788,  2.9772]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.9963,  3.1220]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3985, -2.8501]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7771,  2.9779]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8980,  3.0994]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5663,  2.9046]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8736,  3.0891]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6226,  2.9644]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6489, -3.1403]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5392,  2.8621]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7316,  3.0548]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6281,  3.0192]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3373, -2.7443]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6361, -3.0916]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.5483, -2.1628]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4740, -2.9132]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.9981, -2.5280]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.3476,  2.7524]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7523,  3.0110]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8273,  3.0604]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.9274,  3.1348]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5254, -2.9441]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1773, -2.6411]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.0596,  2.5659]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6974, -3.0344]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8913,  3.0776]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.0948, -2.5768]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6610,  2.9611]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5828,  2.8629]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.4730,  1.9853]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for label in range(100):\n",
    "    x = {}\n",
    "    x['input_ids'] = data[label][\"input_ids\"]\n",
    "    x['token_type_ids'] = data[label][\"token_type_ids\"]\n",
    "    outputs = plain_model(x['input_ids'], x['token_type_ids'])\n",
    "    print(outputs)\n",
    "    count += targets[label] == outputs.argmax()\n",
    "    total += 1\n",
    "count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0014, -1.6264]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5682,  2.9418]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.8672,  2.4340]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1729, -2.6355]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2055,  1.8402]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6816,  3.0014]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3215,  1.2353]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7192,  2.9996]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8530,  3.0773]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5189, -2.9471]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6169,  2.9147]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6550, -2.9953]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3679, -2.8037]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8698,  3.0740]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7491,  1.3029]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.7329, -3.1090]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3681, -2.8039]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1635, -2.6641]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5056, -2.9157]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.9104,  3.0573]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4699, -2.8862]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7428,  3.0556]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.5097, -2.0526]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.0510, -2.6004]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6804, -3.0740]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6611, -3.1032]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.1918,  2.5621]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.0222, -2.5346]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4023, -2.8362]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.0399, -2.5546]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5354, -2.9466]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3500, -2.8403]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7789,  3.0134]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.6365,  2.2060]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.9372,  3.1118]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.5421,  2.0898]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.7349, -3.1063]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.8583, -2.4269]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.7392, -3.0051]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.7680, -3.0666]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.4012,  2.7567]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4581, -2.9231]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7767,  3.0283]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1774,  1.8285]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8772,  3.1095]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7065,  3.0171]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1906,  0.2716]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.0159,  2.6306]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8305,  3.0703]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.6100, -2.2142]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7583,  3.0453]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4991, -2.8787]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.4946, -1.0679]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8817,  3.0804]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.4016, -1.9658]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6998,  2.9333]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1260,  0.0625]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.8578, -2.4066]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.2857, -2.7247]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3104, -2.8145]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4427,  1.2692]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5068, -2.9097]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8926,  3.0806]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4996, -2.8726]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6377, -3.0012]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8546,  3.0752]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.2509,  2.6691]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5408,  1.3560]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6890,  2.9711]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1275, -2.6295]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6786,  2.9770]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.9963,  3.1220]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3985, -2.8501]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7771,  2.9779]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8980,  3.0994]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5663,  2.9046]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8737,  3.0892]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6227,  2.9645]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6490, -3.1403]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5391,  2.8620]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7315,  3.0548]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6280,  3.0192]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.3371, -2.7441]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6360, -3.0916]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.5478, -2.1624]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.4738, -2.9131]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 2.9979, -2.5278]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.3480,  2.7526]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.7525,  3.0110]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8273,  3.0604]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.9274,  3.1347]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.5252, -2.9440]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.1769, -2.6408]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.0583,  2.5649]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.6974, -3.0343]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.8913,  3.0776]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 3.0951, -2.5770]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.6608,  2.9609]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.5826,  2.8628]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.4737,  1.9858]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9100)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for i in range(100):\n",
    "    outputs = bert_model(**data[i])\n",
    "    print(outputs.logits)\n",
    "    count += targets[i] == outputs.logits.argmax()\n",
    "    total += 1\n",
    "count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1327,  1338,  1154,  2049,  1170,  1103,  1207,  7119,  1108,\n",
       "          1123, 18728,   136,   102,  1249,  1104,  1115,  1285,   117,  1103,\n",
       "          1207,  7119,  1123, 18728,  1158,  1103,  2307,  2250,  1338,  1154,\n",
       "          2049,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "390894444956881"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "139932",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "375902760006757",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 4 -- Classification with Encrypted Neural Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
