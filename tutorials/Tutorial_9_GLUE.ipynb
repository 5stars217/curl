{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification with Encrypted Neural Networks\n",
    "\n",
    "In this tutorial, we'll look at how we can achieve the <i>Model Hiding</i> application we discussed in the Introduction. That is, suppose say Alice has a trained model she wishes to keep private, and Bob has some data he wishes to classify while keeping it private. We will see how CrypTen allows Alice and Bob to coordinate and classify the data, while achieving their privacy requirements.\n",
    "\n",
    "To simulate this scenario, we will begin with Alice training a simple neural network on MNIST data. Then we'll see how Alice and Bob encrypt their network and data respectively, classify the encrypted data and finally decrypt the labels.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We first import the `torch` and `crypten` libraries, and initialize `crypten`. We will use a helper script `mnist_utils.py` to split the public MNIST data into Alice's portion and Bob's portion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ProcessGroupGloo.cpp:751] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    }
   ],
   "source": [
    "import crypten\n",
    "import crypten.nn as nn\n",
    "import math\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "crypten.init()\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "forward\n",
      "output_enc\n",
      "output=tensor([[[ 3.2761e-01, -2.6791e-01],\n",
      "         [-1.1989e-01, -3.7218e-01],\n",
      "         [ 1.6673e-01, -2.5415e-01],\n",
      "         [ 2.8795e-01,  2.3799e-01],\n",
      "         [ 2.1680e-01,  1.4282e-02],\n",
      "         [ 2.2366e-01, -2.4100e-01],\n",
      "         [ 5.3741e-02, -2.5662e-01],\n",
      "         [ 1.0928e-01, -1.0956e-02],\n",
      "         [ 2.2964e-01, -1.6299e-01],\n",
      "         [ 2.3883e-01, -3.0801e-01],\n",
      "         [ 1.6910e-01, -8.8898e-02],\n",
      "         [ 3.8145e-01,  4.3121e-02],\n",
      "         [ 2.7388e-01, -3.4944e-01],\n",
      "         [ 1.1966e-01, -5.3261e-01],\n",
      "         [ 2.6854e-01, -8.8959e-02],\n",
      "         [ 4.8885e-01, -3.7724e-01],\n",
      "         [ 2.8743e-01, -1.3196e-01],\n",
      "         [ 3.5741e-01, -6.4240e-02],\n",
      "         [ 8.6365e-03,  4.3289e-02],\n",
      "         [-8.2016e-02,  8.9615e-02],\n",
      "         [ 4.1743e-01, -4.1037e-01],\n",
      "         [ 3.1194e-01, -3.7967e-01],\n",
      "         [ 8.5495e-02, -4.7932e-01],\n",
      "         [ 3.5640e-01, -1.0381e-01],\n",
      "         [ 6.4499e-02,  2.8355e-01],\n",
      "         [ 2.1394e-01, -3.0057e-01],\n",
      "         [ 1.1969e-01, -3.0655e-01],\n",
      "         [ 2.2293e-01,  1.3695e-01],\n",
      "         [ 6.2856e-01, -3.2483e-01],\n",
      "         [ 2.9724e-01,  4.1611e-02],\n",
      "         [ 4.8662e-01,  9.6191e-02],\n",
      "         [ 7.6379e-01, -1.4734e-01],\n",
      "         [ 5.1987e-02, -3.6655e-01],\n",
      "         [ 3.4154e-01, -1.8852e-01],\n",
      "         [ 4.0770e-01, -2.0979e-01],\n",
      "         [ 3.0513e-01, -3.0518e-05],\n",
      "         [ 1.5994e-01, -1.2350e-01],\n",
      "         [ 3.6444e-01, -1.7850e-01],\n",
      "         [-3.1418e-01, -1.1459e-02],\n",
      "         [ 3.6613e-01, -3.8907e-01],\n",
      "         [ 4.2014e-01,  1.5836e-01],\n",
      "         [-4.1718e-02, -3.9383e-02],\n",
      "         [-4.6860e-02, -1.0966e-01],\n",
      "         [ 2.8212e-01, -3.8460e-01],\n",
      "         [ 3.1491e-01, -5.2806e-01],\n",
      "         [ 4.9179e-02,  1.5045e-01],\n",
      "         [ 1.0249e-01, -1.8422e-01],\n",
      "         [ 1.4536e-01, -2.1393e-01],\n",
      "         [ 3.6571e-01, -1.8181e-01],\n",
      "         [ 2.3586e-01,  1.1017e-02],\n",
      "         [ 3.2182e-01, -2.8754e-01],\n",
      "         [ 2.0139e-01, -1.4325e-01],\n",
      "         [-1.8581e-01,  2.0102e-01],\n",
      "         [ 3.0777e-01, -2.5011e-01],\n",
      "         [ 2.8137e-02, -4.3889e-01],\n",
      "         [ 5.4474e-03, -3.8724e-01],\n",
      "         [-1.3814e-01, -8.9839e-01],\n",
      "         [ 1.4996e-01, -3.6838e-01],\n",
      "         [ 4.2316e-01, -1.1032e-02],\n",
      "         [ 2.2583e-01, -2.6213e-01],\n",
      "         [ 5.3955e-02,  1.8575e-01],\n",
      "         [ 1.1542e-01,  3.3310e-02],\n",
      "         [-2.5711e-02, -2.1173e-01],\n",
      "         [-4.8843e-02, -2.7654e-01]]])\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"invalid heads and embedding dimension\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.search_dim = embed_dim // num_heads\n",
    "\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, self.search_dim).transpose(1, 2)\n",
    "\n",
    "        attn = q.matmul(k_t) / math.sqrt(q.size(-1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        y = attn.matmul(v)\n",
    "        y = y.transpose(1, 2)\n",
    "        y = y.reshape(batch_size, seq_len, self.embed_dim)\n",
    "        return y\n",
    "\n",
    "\n",
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(BertBlock, self).__init__()\n",
    "        embed_dim = embed_dim\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = Attention(embed_dim, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x + self.attn(x))\n",
    "        x = self.ln2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_blocks, vocab_size, seq_len, full=True):\n",
    "        super(Bert, self).__init__()\n",
    "        self.full = full\n",
    "        if full:\n",
    "            self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
    "        self.ln = nn.LayerNorm\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[BertBlock(embed_dim, num_heads) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        if full:\n",
    "            self.pooler = nn.Linear(embed_dim, embed_dim)\n",
    "            self.tanh = nn.TANH()\n",
    "            self.classifier = nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        if self.full:\n",
    "            tok_embedding = self.tok_embed(x)\n",
    "            pos_embedding = self.pos_embed(x)[:, :x.size()[1], :]\n",
    "            x = tok_embedding + pos_embedding\n",
    "        x = self.ln(x)\n",
    "        x = self.blocks(x)\n",
    "        if self.full:\n",
    "            x = self.pooler(x)\n",
    "            x = self.tanh(x)\n",
    "            x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = Bert(768, 12, 12, 30522, 1024, True) # bert base 13.5s\n",
    "model.encrypt(src=0)\n",
    "\n",
    "# Load data to Bob\n",
    "print('loading data')\n",
    "data_enc = crypten.cryptensor(torch.arange(64).reshape(1, 64))\n",
    "\n",
    "# Classify the encrypted data\n",
    "model.eval()\n",
    "print(\"forward\")\n",
    "output_enc = model(data_enc)\n",
    "print('output_enc')\n",
    "# Compute the accuracy\n",
    "output = output_enc.get_plain_text()\n",
    "print(f\"{output=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight: torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight: torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight: torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight: torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
      "bert.pooler.dense.weight: torch.Size([768, 768])\n",
      "bert.pooler.dense.bias: torch.Size([768])\n",
      "classifier.weight: torch.Size([2, 768])\n",
      "classifier.bias: torch.Size([2])\n",
      "n_weight=109380864, n_bias=102914, n_param=109483778\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Access the model's weights\n",
    "weights = bert_model.state_dict()\n",
    "\n",
    "# Modify the weights or perform any operation you desire\n",
    "# Example: Print the shape of each weight tensor\n",
    "o = 0\n",
    "b = 0\n",
    "for name, weight in weights.items():\n",
    "    if \"weight\" in str(name):\n",
    "        print(f\"{name}: {weight.size()}\")\n",
    "        p = 1\n",
    "        for w in weight.size():\n",
    "            p *= w\n",
    "        o += p\n",
    "    elif \"bias\" in str(name):\n",
    "        print(f\"{name}: {weight.size()}\")\n",
    "        p = 1\n",
    "        for w in weight.size():\n",
    "            p *= w\n",
    "        b += p\n",
    "    else:\n",
    "        print(f\"else {name}: {weight.size()}\")\n",
    "print(f\"n_weight={o}, n_bias={b}, n_param={o+b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert(768, 12, 12, 30522, 1024, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tok_embed.weight = weights[\"bert.embeddings.word_embeddings.weight\"]\n",
    "model.pos_embed.weight = weights[\"bert.embeddings.position_embeddings.weight\"][None, :, :]\n",
    "model.ln.weight = weights[\"bert.embeddings.LayerNorm.weight\"]\n",
    "model.ln.bias = weights[\"bert.embeddings.LayerNorm.bias\"]\n",
    "for m in range(len(model.blocks._modules)):\n",
    "    layer = \"bert.encoder.layer.\"\n",
    "    model.blocks._modules[str(m)].attn.query.weight = weights[layer+str(m)+\".attention.self.query.weight\"]\n",
    "    model.blocks._modules[str(m)].attn.query.bias = weights[layer+str(m)+\".attention.self.query.bias\"]\n",
    "    model.blocks._modules[str(m)].attn.key.weight = weights[layer+str(m)+\".attention.self.key.weight\"]\n",
    "    model.blocks._modules[str(m)].attn.key.bias = weights[layer+str(m)+\".attention.self.key.bias\"]\n",
    "    model.blocks._modules[str(m)].attn.value.weight = weights[layer+str(m)+\".attention.self.value.weight\"]\n",
    "    model.blocks._modules[str(m)].attn.value.bias = weights[layer+str(m)+\".attention.self.value.bias\"]\n",
    "    model.blocks._modules[str(m)].attn.proj.weight = weights[layer+str(m)+\".attention.output.dense.weight\"] # .t()\n",
    "    model.blocks._modules[str(m)].attn.proj.bias = weights[layer+str(m)+\".attention.output.dense.bias\"]\n",
    "    model.blocks._modules[str(m)].ln1.weight = weights[layer+str(m)+\".attention.output.LayerNorm.weight\"]\n",
    "    model.blocks._modules[str(m)].ln1.bias = weights[layer+str(m)+\".attention.output.LayerNorm.bias\"]\n",
    "    model.blocks._modules[str(m)].ff._modules['0'].weight = weights[layer+str(m)+\".intermediate.dense.weight\"] # .t()\n",
    "    model.blocks._modules[str(m)].ff._modules['0'].bias = weights[layer+str(m)+\".intermediate.dense.bias\"]\n",
    "    model.blocks._modules[str(m)].ff._modules['2'].weight = weights[layer+str(m)+\".output.dense.weight\"] # .t()\n",
    "    model.blocks._modules[str(m)].ff._modules['2'].bias = weights[layer+str(m)+\".output.dense.bias\"]\n",
    "    model.blocks._modules[str(m)].ln2.weight = weights[layer+str(m)+\".output.LayerNorm.weight\"]\n",
    "    model.blocks._modules[str(m)].ln2.bias = weights[layer+str(m)+\".output.LayerNorm.bias\"]\n",
    "model.pooler.weight = weights[\"bert.pooler.dense.weight\"] # .t()\n",
    "model.pooler.bias = weights[\"bert.pooler.dense.bias\"]\n",
    "model.classifier.weight = weights[\"classifier.weight\"] # .t()\n",
    "model.classifier.bias = weights[\"classifier.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2915, -0.1456]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = bert_model(**inputs)\n",
    "realput = outputs\n",
    "print(realput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def load_tsv(data_file, max_seq_len, delimiter='\\t'):\n",
    "    '''Load a tsv '''\n",
    "    sentences = []\n",
    "    targets = []\n",
    "    with codecs.open(data_file, 'r', 'utf-8') as data_fh:\n",
    "        for _ in range(1):\n",
    "            data_fh.readline()\n",
    "        for row in data_fh:\n",
    "            row = row.strip().split(delimiter)\n",
    "            sentence = \"Question: \" + row[1] + \"\\nAnswer Text: \" + row[2]\n",
    "            sentences.append(tokenizer(sentence, return_tensors=\"pt\"))\n",
    "            targets.append(1*(row[3] == \"not_entailment\"))\n",
    "    return sentences, targets\n",
    "\n",
    "data, targets = load_tsv(\"GLUE-baselines/glue_data/QNLI/train.tsv\", 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4940)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for label in range(1000):\n",
    "    outputs = bert_model(**data[label])\n",
    "    count += targets[label] == outputs.logits.argmax()\n",
    "    total += 1\n",
    "count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "forward\n",
      "output_enc\n",
      "output.shape=torch.Size([1, 69, 2])\n",
      "output=tensor([[[-0.1482,  0.2658],\n",
      "         [-0.2622,  0.2909],\n",
      "         [-0.4788, -0.2028],\n",
      "         [-0.3937,  0.0043],\n",
      "         [-0.4800, -0.0475],\n",
      "         [-0.3161, -0.1730],\n",
      "         [-0.2097,  0.2421],\n",
      "         [-0.5458, -0.2761],\n",
      "         [-0.4550, -0.0066],\n",
      "         [-0.3025, -0.0943],\n",
      "         [-0.5511, -0.4168],\n",
      "         [-0.1581,  0.2003],\n",
      "         [-0.5367, -0.2714],\n",
      "         [-0.4930, -0.0416],\n",
      "         [-0.3660,  0.0542],\n",
      "         [-0.4442, -0.1850],\n",
      "         [-0.4354, -0.2316],\n",
      "         [-0.3931, -0.0638],\n",
      "         [-0.0797,  0.4111],\n",
      "         [-0.3819, -0.1658],\n",
      "         [-0.1856,  0.0288],\n",
      "         [-0.4155, -0.2120],\n",
      "         [-0.2390,  0.1315],\n",
      "         [-0.2065,  0.0993],\n",
      "         [-0.3333, -0.1773],\n",
      "         [-0.5713, -0.1519],\n",
      "         [-0.4734, -0.2253],\n",
      "         [-0.4576, -0.1731],\n",
      "         [-0.3396,  0.0236],\n",
      "         [-0.5201, -0.0949],\n",
      "         [-0.4643, -0.2706],\n",
      "         [-0.5551, -0.0987],\n",
      "         [-0.0946,  0.3129],\n",
      "         [-0.2258,  0.3081],\n",
      "         [-0.5072, -0.2103],\n",
      "         [-0.5091, -0.1237],\n",
      "         [-0.2909,  0.1749],\n",
      "         [-0.4561,  0.0093],\n",
      "         [-0.4004, -0.2148],\n",
      "         [-0.6260, -0.2944],\n",
      "         [-0.1664,  0.2667],\n",
      "         [-0.5207,  0.0043],\n",
      "         [-0.5101, -0.1130],\n",
      "         [-0.4970, -0.2236],\n",
      "         [-0.4179,  0.1067],\n",
      "         [-0.4784, -0.3195],\n",
      "         [-0.2337,  0.0590],\n",
      "         [-0.4184, -0.0182],\n",
      "         [-0.5387, -0.1517],\n",
      "         [-0.6225, -0.1313],\n",
      "         [-0.3163,  0.0457],\n",
      "         [-0.0129,  0.4073],\n",
      "         [-0.4826, -0.2372],\n",
      "         [-0.0017,  0.3986],\n",
      "         [-0.4132, -0.0733],\n",
      "         [-0.3895, -0.1340],\n",
      "         [-0.3912,  0.0111],\n",
      "         [ 0.0667,  0.3805],\n",
      "         [-0.6143, -0.1988],\n",
      "         [-0.4987, -0.0630],\n",
      "         [-0.5453, -0.2733],\n",
      "         [-0.2504,  0.3682],\n",
      "         [-0.3082,  0.0532],\n",
      "         [-0.3809, -0.0429],\n",
      "         [-0.1551,  0.4130],\n",
      "         [ 0.0139,  0.3500],\n",
      "         [-0.4301, -0.0626],\n",
      "         [ 0.0129,  0.3426],\n",
      "         [-0.3839, -0.1457]]])\n"
     ]
    }
   ],
   "source": [
    "model.encrypt(src=0)\n",
    "# Load data to Bob\n",
    "print('loading data')\n",
    "# data_enc = crypten.load_from_party('/tmp/bob_test.pth', src=ALICE)\n",
    "x = crypten.cryptensor(data[0]['input_ids']).reshape(1, -1)\n",
    "# Classify the encrypted data\n",
    "model.eval()\n",
    "print(\"forward\")\n",
    "x = model(x)\n",
    "print('output_enc')\n",
    "# Compute the accuracy\n",
    "output = x.get_plain_text()\n",
    "print(f\"{output.shape=}\")\n",
    "print(f\"{output=}\")"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "390894444956881"
  },
  "disseminate_notebook_info": {
   "bento_version": "20190826-030256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/shobha/fbsource/fbcode/bento/kernels/local/cryptenk/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "139932",
   "others_can_edit": true,
   "reviewers": "",
   "revision_id": "375902760006757",
   "tags": "",
   "tasks": "",
   "title": "Tutorial 4 -- Classification with Encrypted Neural Networks"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
